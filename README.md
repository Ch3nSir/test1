# PISCO Reference Implementation

This repository contains a reference implementation of the methodology
outlined in *"PISCO: Pretty Simple Compression for Retrieval-Augmented
Generation"*.  It provides:

- A document compressor that produces a fixed number of memory tokens via
  LoRA-adapted Transformer encoders.
- A decoder that consumes the memory tokens together with a question to
  produce an answer.
- A sequence-level knowledge distillation loop that supervises the system
  with responses generated by a teacher model.
- A command line training entry point that reads JSON configuration files
  and JSONL datasets.

## Project structure

```
pisco/
├── config.py            # Dataclass-based configuration objects
├── data.py              # Lightweight QA dataset helpers
├── distillation.py      # Sequence-level distillation training loop
└── models/
    ├── compressor.py    # Document compressor with LoRA adapters
    ├── decoder.py       # Memory-aware decoder
    ├── lora.py          # Generic LoRA utilities
    └── pisco_model.py   # High-level wrapper exposing ``forward``/``answer``
scripts/
└── train.py             # CLI to launch distillation
```

## Usage

1. **Create a dataset** in JSON Lines format where every row contains a
   ``document`` and ``question`` field.  Optionally include
   ``teacher_answer`` when curated responses are already available.

2. **Prepare a configuration** file such as:

```json
{
  "compressor": {
    "model_name_or_path": "facebook/contriever",
    "memory_tokens": 32,
    "lora": {"rank": 16, "alpha": 32, "dropout": 0.05}
  },
  "decoder": {
    "model_name_or_path": "meta-llama/Llama-2-7b-hf",
    "max_new_tokens": 128,
    "lora": {"rank": 16, "alpha": 32}
  },
  "teacher_model_name_or_path": "meta-llama/Llama-2-13b-chat-hf",
  "learning_rate": 1e-4,
  "batch_size": 128,
  "max_steps": 2000
}
```

3. **Launch training**:

```bash
python scripts/train.py config.json --dataset data/train.jsonl --output checkpoints/pisco.pt
```

During training the teacher model will auto-generate missing labels by
concatenating each question with its corresponding document and sampling a
response.  The resulting checkpoint stores both the trained PISCO weights
and summary statistics of the run.

## Notes

- All transformer models are loaded via the Hugging Face `transformers`
  library; make sure the required weights are available locally or via the
  model hub.
- Only LoRA parameters are marked as trainable, mirroring the efficient
  fine-tuning approach described in the paper.
- Mixed-precision training is enabled by default (BF16) and can be
  switched off by setting ``"mixed_precision": null`` in the configuration.

